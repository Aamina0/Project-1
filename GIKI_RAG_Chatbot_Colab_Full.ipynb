{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aamina0/Project-1/blob/main/GIKI_RAG_Chatbot_Colab_Full.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e3055acc",
      "metadata": {
        "id": "e3055acc"
      },
      "source": [
        "# GIKI RAG Chatbot â€” Colab Notebook (Full)\n",
        "\n",
        "This notebook contains a full Retrieval-Augmented Generation (RAG) pipeline:\n",
        "\n",
        "- Document ingestion (PDF/DOCX/TXT)\n",
        "- Chunking\n",
        "- Multilingual embeddings (intfloat/multilingual-e5-base)\n",
        "- FAISS vector store with persistence\n",
        "- Generation with Flan-T5 (paraphrasing prompt)\n",
        "- English/Urdu toggle using Helsinki-NLP Marian models\n",
        "- Streamlit UI with pyngrok launcher for Colab\n",
        "- Conversation export to PDF\n",
        "\n",
        "**Run cells top-to-bottom in Colab.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "7317b3cb",
      "metadata": {
        "id": "7317b3cb"
      },
      "outputs": [],
      "source": [
        "# Install dependencies (run in Colab)\n",
        "!pip -q install --upgrade pip\n",
        "!pip -q install sentence-transformers faiss-cpu transformers accelerate pymupdf python-docx pypdf reportlab streamlit pyngrok>=5.0.0 langdetect"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# âœ… 2) Mount Google Drive and set project working directory\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "PROJECT_DIR = '/content/drive/MyDrive/Project3'\n",
        "import os\n",
        "os.makedirs(PROJECT_DIR, exist_ok=True)\n",
        "%cd \"$PROJECT_DIR\"\n",
        "print('Working directory:', PROJECT_DIR)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ElT1m-op-3N",
        "outputId": "91240671-771c-4501-fc4e-d225a6d68aad"
      },
      "id": "2ElT1m-op-3N",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/MyDrive/Project3\n",
            "Working directory: /content/drive/MyDrive/Project3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "4e3506b6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4e3506b6",
        "outputId": "d7a0e814-c61b-4f34-ae46-30fe0a787930"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cpu\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Imports & config\n",
        "import os, io, pickle, time, textwrap\n",
        "from pathlib import Path\n",
        "\n",
        "import faiss\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline, MarianMTModel, MarianTokenizer\n",
        "\n",
        "import fitz  # pymupdf\n",
        "import docx\n",
        "from pypdf import PdfReader\n",
        "\n",
        "from reportlab.lib.pagesizes import letter\n",
        "from reportlab.pdfgen import canvas\n",
        "\n",
        "try:\n",
        "    from langdetect import detect as lang_detect\n",
        "except Exception:\n",
        "    lang_detect = None\n",
        "\n",
        "# Paths\n",
        "BASE_DIR = Path.cwd()\n",
        "DATA_DIR = BASE_DIR / \"data\"\n",
        "INDEX_DIR = BASE_DIR / \"faiss_store\"\n",
        "APP_DIR = BASE_DIR / \"app\"\n",
        "DATA_DIR.mkdir(exist_ok=True)\n",
        "INDEX_DIR.mkdir(exist_ok=True)\n",
        "APP_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "INDEX_FILE = INDEX_DIR / \"index.faiss\"\n",
        "META_FILE = INDEX_DIR / \"metadata.pkl\"\n",
        "\n",
        "# Models\n",
        "EMBEDDING_MODEL_NAME = \"intfloat/multilingual-e5-base\"\n",
        "GEN_MODEL_NAME = \"google/flan-t5-large\"\n",
        "\n",
        "device = 0 if torch.cuda.is_available() else -1\n",
        "print(\"Device:\", \"cuda\" if device==0 else \"cpu\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "80f871b8",
      "metadata": {
        "id": "80f871b8"
      },
      "outputs": [],
      "source": [
        "# Optimized document ingestion utilities\n",
        "def extract_text_from_pdf_pymupdf(pdf_path: str) -> str:\n",
        "    text = []\n",
        "    with fitz.open(pdf_path) as doc:\n",
        "        for page in doc:\n",
        "            text.append(page.get_text())\n",
        "    return \"\\n\".join(text)\n",
        "\n",
        "def extract_text_from_pdf_pypdf(pdf_path: str) -> str:\n",
        "    text = []\n",
        "    with open(pdf_path, \"rb\") as f:\n",
        "        reader = PdfReader(f)\n",
        "        for page in reader.pages:\n",
        "            text.append(page.extract_text() or \"\")\n",
        "    return \"\\n\".join(text)\n",
        "\n",
        "def extract_text_from_docx(docx_path: str) -> str:\n",
        "    d = docx.Document(docx_path)\n",
        "    return \"\\n\".join([p.text for p in d.paragraphs])\n",
        "\n",
        "def extract_text_from_txt(txt_path: str) -> str:\n",
        "    return Path(txt_path).read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
        "\n",
        "def load_documents(filepaths, max_pages=50):\n",
        "    texts = []\n",
        "    for p in filepaths:\n",
        "        p = str(p)\n",
        "        if p.lower().endswith(\".pdf\"):\n",
        "            try:\n",
        "                # Extract only first max_pages to prevent excessive processing\n",
        "                txt = []\n",
        "                with fitz.open(p) as doc:\n",
        "                    for i, page in enumerate(doc):\n",
        "                        if i >= max_pages:\n",
        "                            break\n",
        "                        txt.append(page.get_text())\n",
        "                txt = \"\\n\".join(txt)\n",
        "                if not txt.strip():\n",
        "                    txt = extract_text_from_pdf_pypdf(p)\n",
        "            except Exception:\n",
        "                txt = extract_text_from_pdf_pypdf(p)\n",
        "        elif p.lower().endswith(\".docx\"):\n",
        "            txt = extract_text_from_docx(p)\n",
        "        elif p.lower().endswith(\".txt\"):\n",
        "            txt = extract_text_from_txt(p)\n",
        "        else:\n",
        "            print(\"Skipping unsupported file:\", p)\n",
        "            continue\n",
        "        if txt.strip():\n",
        "            texts.append(txt)\n",
        "    return \"\\n\".join(texts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "6455eca5",
      "metadata": {
        "id": "6455eca5"
      },
      "outputs": [],
      "source": [
        "# Optimized chunking utility (by characters instead of words for faster processing)\n",
        "def chunk_text(text: str, chunk_size_chars=2000, overlap_chars=200):\n",
        "    chunks = []\n",
        "    for i in range(0, len(text), max(1, chunk_size_chars - overlap_chars)):\n",
        "        chunk = text[i:i+chunk_size_chars]\n",
        "        if chunk.strip():\n",
        "            chunks.append(chunk)\n",
        "    return chunks\n",
        "\n",
        "# Optimized FAISS building with progress indication\n",
        "def build_or_load_faiss(chunks):\n",
        "    if INDEX_FILE.exists() and META_FILE.exists():\n",
        "        print(\"[FAISS] Loading existing index and metadata...\")\n",
        "        index = faiss.read_index(str(INDEX_FILE))\n",
        "        with open(META_FILE, \"rb\") as f:\n",
        "            metadata = pickle.load(f)\n",
        "        return index, metadata\n",
        "\n",
        "    print(\"[FAISS] Building new index...\")\n",
        "\n",
        "    # Process in smaller batches to avoid memory issues\n",
        "    batch_size = 16  # Reduced batch size\n",
        "    all_embs = []\n",
        "\n",
        "    for i in range(0, len(chunks), batch_size):\n",
        "        batch = chunks[i:i+batch_size]\n",
        "        print(f\"Processing batch {i//batch_size + 1}/{(len(chunks)-1)//batch_size + 1}\")\n",
        "        embs = embedding_model.encode(batch, batch_size=8, show_progress_bar=False,\n",
        "                                     convert_to_numpy=True, normalize_embeddings=True)\n",
        "        all_embs.append(embs)\n",
        "\n",
        "    embs = np.vstack(all_embs)\n",
        "    dim = embs.shape[1]\n",
        "    index = faiss.IndexFlatIP(dim)\n",
        "    index.add(embs)\n",
        "    metadata = {i: {\"text\": chunks[i]} for i in range(len(chunks))}\n",
        "\n",
        "    faiss.write_index(index, str(INDEX_FILE))\n",
        "    with open(META_FILE, \"wb\") as f:\n",
        "        pickle.dump(metadata, f)\n",
        "\n",
        "    print(\"[FAISS] Saved index and metadata.\")\n",
        "    return index, metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "4d694da2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4d694da2",
        "outputId": "6216a565-953f-4b10-ca6d-6c94d5eccec1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading embedding model (this may take time)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Embedding model and FAISS persistence\n",
        "print(\"Loading embedding model (this may take time)...\")\n",
        "embedding_model = SentenceTransformer(EMBEDDING_MODEL_NAME, device=('cuda' if device==0 else 'cpu'))\n",
        "\n",
        "def build_or_load_faiss(chunks):\n",
        "    if INDEX_FILE.exists() and META_FILE.exists():\n",
        "        print(\"[FAISS] Loading existing index and metadata...\")\n",
        "        index = faiss.read_index(str(INDEX_FILE))\n",
        "        with open(META_FILE, \"rb\") as f:\n",
        "            metadata = pickle.load(f)\n",
        "        return index, metadata\n",
        "    print(\"[FAISS] Building new index...\")\n",
        "    embs = embedding_model.encode(chunks, batch_size=64, show_progress_bar=True, convert_to_numpy=True, normalize_embeddings=True)\n",
        "    dim = embs.shape[1]\n",
        "    index = faiss.IndexFlatIP(dim)\n",
        "    index.add(embs)\n",
        "    metadata = {i: {\"text\": chunks[i]} for i in range(len(chunks))}\n",
        "    faiss.write_index(index, str(INDEX_FILE))\n",
        "    with open(META_FILE, \"wb\") as f:\n",
        "        pickle.dump(metadata, f)\n",
        "    print(\"[FAISS] Saved index and metadata.\")\n",
        "    return index, metadata\n",
        "\n",
        "def load_faiss_only():\n",
        "    if INDEX_FILE.exists() and META_FILE.exists():\n",
        "        index = faiss.read_index(str(INDEX_FILE))\n",
        "        with open(META_FILE, \"rb\") as f:\n",
        "            metadata = pickle.load(f)\n",
        "        return index, metadata\n",
        "    raise FileNotFoundError(\"No FAISS index found. Build it first.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "0dc7071b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0dc7071b",
        "outputId": "6b6aeed6-971d-4179-e5f6-5673e0a1204e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading generation model (this may take time)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        }
      ],
      "source": [
        "# Generation model (Flan-T5) and paraphrase prompt\n",
        "print(\"Loading generation model (this may take time)...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(GEN_MODEL_NAME)\n",
        "gen_model = AutoModelForSeq2SeqLM.from_pretrained(GEN_MODEL_NAME, device_map=\"auto\")\n",
        "generator = pipeline(\"text2text-generation\", model=gen_model, tokenizer=tokenizer, max_new_tokens=300, temperature=0.3, top_p=0.95)\n",
        "\n",
        "PARAPHRASE_PROMPT = \"\"\"You are an assistant for GIKI students.\n",
        "Answer the question using ONLY the provided context. Do NOT copy-paste; paraphrase and summarize in your own words.\n",
        "If the answer cannot be found in the context, say you don't know.\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "def generate_answer(question: str, retrieved_text: str) -> str:\n",
        "    prompt = PARAPHRASE_PROMPT.format(question=question, context=retrieved_text)\n",
        "    out = generator(prompt)[0][\"generated_text\"].strip()\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "14750b91",
      "metadata": {
        "id": "14750b91"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Translation helpers using MarianMT\n",
        "_mt_cache = {}\n",
        "def _load_translator(src_tgt):\n",
        "    if src_tgt in _mt_cache:\n",
        "        return _mt_cache[src_tgt]\n",
        "    model_name = {\"ur-en\": \"Helsinki-NLP/opus-mt-ur-en\", \"en-ur\": \"Helsinki-NLP/opus-mt-en-ur\"}[src_tgt]\n",
        "    tok = MarianTokenizer.from_pretrained(model_name)\n",
        "    mod = MarianMTModel.from_pretrained(model_name)\n",
        "    _mt_cache[src_tgt] = (tok, mod)\n",
        "    return tok, mod\n",
        "\n",
        "def translate(text: str, src_tgt: str) -> str:\n",
        "    tok, mod = _load_translator(src_tgt)\n",
        "    batch = tok([text], return_tensors=\"pt\", padding=True, truncation=True)\n",
        "    with torch.no_grad():\n",
        "        gen = mod.generate(**batch, max_new_tokens=400)\n",
        "    return tok.batch_decode(gen, skip_special_tokens=True)[0]\n",
        "\n",
        "def detect_lang(text: str) -> str:\n",
        "    if lang_detect:\n",
        "        try:\n",
        "            code = lang_detect(text)\n",
        "            return \"ur\" if code.startswith(\"ur\") else \"en\"\n",
        "        except Exception:\n",
        "            pass\n",
        "    for ch in text:\n",
        "        if \"\\u0600\" <= ch <= \"\\u06FF\":\n",
        "            return \"ur\"\n",
        "    return \"en\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "3f8f32bd",
      "metadata": {
        "id": "3f8f32bd"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Retrieval + answer function\n",
        "index = None\n",
        "metadata = None\n",
        "\n",
        "def retrieve_context(query_en: str, top_k=4):\n",
        "    q_emb = embedding_model.encode([query_en], convert_to_numpy=True, normalize_embeddings=True)\n",
        "    D, I = index.search(q_emb, top_k)\n",
        "    chunks = []\n",
        "    for idx in I[0]:\n",
        "        if int(idx) in metadata:\n",
        "            chunks.append(metadata[int(idx)][\"text\"])\n",
        "    return \"\\n\\n\".join(chunks)\n",
        "\n",
        "def answer_question(query: str, lang='en', top_k=4):\n",
        "    # translate if needed, retrieve, generate, translate back\n",
        "    if lang == 'ur':\n",
        "        query_en = translate(query, \"ur-en\")\n",
        "    else:\n",
        "        query_en = query\n",
        "    ctx = retrieve_context(query_en, top_k=top_k)\n",
        "    ans_en = generate_answer(query_en, ctx)\n",
        "    if lang == 'ur':\n",
        "        return translate(ans_en, \"en-ur\"), ctx\n",
        "    return ans_en, ctx\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "bac880a1",
      "metadata": {
        "id": "bac880a1"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Evaluation & export\n",
        "def cosine_sim(a,b):\n",
        "    a = np.array(a) / (np.linalg.norm(a)+1e-8)\n",
        "    b = np.array(b) / (np.linalg.norm(b)+1e-8)\n",
        "    return float(np.dot(a,b))\n",
        "\n",
        "def evaluate_answer(answer: str, context: str):\n",
        "    if not answer.strip() or not context.strip():\n",
        "        return 0.0\n",
        "    emb_a = embedding_model.encode([answer], convert_to_numpy=True, normalize_embeddings=True)[0]\n",
        "    emb_c = embedding_model.encode([context], convert_to_numpy=True, normalize_embeddings=True)[0]\n",
        "    return cosine_sim(emb_a, emb_c)\n",
        "\n",
        "def save_conversation_to_pdf(lines, filename=\"conversation.pdf\"):\n",
        "    c = canvas.Canvas(filename, pagesize=letter)\n",
        "    width, height = letter\n",
        "    x, y = 40, height-40\n",
        "    wrap = 95\n",
        "    for line in lines:\n",
        "        for l in textwrap.wrap(line, wrap):\n",
        "            if y < 60:\n",
        "                c.showPage()\n",
        "                y = height-40\n",
        "            c.drawString(x, y, l)\n",
        "            y -= 14\n",
        "    c.save()\n",
        "    return filename\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "76b4d349",
      "metadata": {
        "id": "76b4d349"
      },
      "outputs": [],
      "source": [
        "# Build or load FAISS index from files in data/\n",
        "from glob import glob\n",
        "def discover_files(extensions=(\".pdf\", \".docx\", \".txt\"), limit=3):  # Reduced limit\n",
        "    found = []\n",
        "    for ext in extensions:\n",
        "        found.extend(glob(str(DATA_DIR / f\"*{ext}\")))\n",
        "    found = sorted(found)[:limit]\n",
        "    return found\n",
        "\n",
        "def build_index_from_data():\n",
        "    global index, metadata\n",
        "    files = discover_files()\n",
        "    print(\"Files:\", files)\n",
        "\n",
        "    # Limit processing to first 20 pages of PDFs\n",
        "    text = load_documents(files, max_pages=20)\n",
        "    chunks = chunk_text(text, 2000, 200)  # Use character-based chunking\n",
        "\n",
        "    # Show progress\n",
        "    print(f\"Processing {len(chunks)} chunks...\")\n",
        "    index, metadata = build_or_load_faiss(chunks)\n",
        "    print(\"Index ready. Chunks:\", len(metadata))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "df1c65bd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "df1c65bd",
        "outputId": "f061f5e6-7120-48a5-d98a-f1216fc6ebfa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Streamlit app created at: /content/drive/MyDrive/Project3/app/app.py\n"
          ]
        }
      ],
      "source": [
        "# Create the optimized Streamlit app file\n",
        "app_content = '''\n",
        "import streamlit as st\n",
        "from pathlib import Path\n",
        "import faiss, pickle, textwrap\n",
        "import torch\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline, MarianMTModel, MarianTokenizer\n",
        "import fitz, docx\n",
        "from reportlab.lib.pagesizes import letter\n",
        "from reportlab.pdfgen import canvas\n",
        "import numpy as np\n",
        "import tempfile\n",
        "import os\n",
        "import time\n",
        "\n",
        "BASE_DIR = Path.cwd()\n",
        "DATA_DIR = BASE_DIR / \"data\"\n",
        "INDEX_DIR = BASE_DIR / \"faiss_store\"\n",
        "INDEX_FILE = INDEX_DIR / \"index.faiss\"\n",
        "META_FILE  = INDEX_DIR / \"metadata.pkl\"\n",
        "DATA_DIR.mkdir(exist_ok=True)\n",
        "INDEX_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "EMBEDDING_MODEL_NAME = \"intfloat/multilingual-e5-base\"\n",
        "GEN_MODEL_NAME = \"google/flan-t5-large\"\n",
        "\n",
        "device = 0 if torch.cuda.is_available() else -1\n",
        "\n",
        "@st.cache_resource\n",
        "def get_embedding_model():\n",
        "    return SentenceTransformer(EMBEDDING_MODEL_NAME, device=('cuda' if device==0 else 'cpu'))\n",
        "\n",
        "@st.cache_resource\n",
        "def get_generator():\n",
        "    tok = AutoTokenizer.from_pretrained(GEN_MODEL_NAME)\n",
        "    mod = AutoModelForSeq2SeqLM.from_pretrained(GEN_MODEL_NAME)\n",
        "    return pipeline(\n",
        "        \"text2text-generation\",\n",
        "        model=mod,\n",
        "        tokenizer=tok,\n",
        "        max_new_tokens=300,\n",
        "        temperature=0.3,\n",
        "        top_p=0.95\n",
        "    )\n",
        "\n",
        "_mt_cache = {}\n",
        "def get_translator(src_tgt):\n",
        "    if src_tgt in _mt_cache:\n",
        "        return _mt_cache[src_tgt]\n",
        "    model_name = {\"ur-en\": \"Helsinki-NLP/opus-mt-ur-en\", \"en-ur\": \"Helsinki-NLP/opus-mt-en-ur\"}[src_tgt]\n",
        "    tok = MarianTokenizer.from_pretrained(model_name)\n",
        "    mod = MarianMTModel.from_pretrained(model_name)\n",
        "    _mt_cache[src_tgt] = (tok, mod)\n",
        "    return tok, mod\n",
        "\n",
        "def translate(text, src_tgt):\n",
        "    tok, mod = get_translator(src_tgt)\n",
        "    batch = tok([text], return_tensors=\"pt\", padding=True, truncation=True)\n",
        "    with torch.no_grad():\n",
        "        gen = mod.generate(**batch, max_new_tokens=400)\n",
        "    return tok.batch_decode(gen, skip_special_tokens=True)[0]\n",
        "\n",
        "# Optimized chunking function\n",
        "def chunk_text_local(text: str, chunk_size_chars=2000, overlap_chars=200):\n",
        "    chunks = []\n",
        "    for i in range(0, len(text), max(1, chunk_size_chars - overlap_chars)):\n",
        "        chunk = text[i:i+chunk_size_chars]\n",
        "        if chunk.strip():\n",
        "            chunks.append(chunk)\n",
        "    return chunks\n",
        "\n",
        "def load_faiss_local():\n",
        "    if INDEX_FILE.exists() and META_FILE.exists():\n",
        "        idx = faiss.read_index(str(INDEX_FILE))\n",
        "        with open(META_FILE, \"rb\") as f:\n",
        "            meta = pickle.load(f)\n",
        "        return idx, meta\n",
        "    return None, None\n",
        "\n",
        "def save_faiss_local(index, metadata):\n",
        "    INDEX_DIR.mkdir(exist_ok=True)\n",
        "    faiss.write_index(index, str(INDEX_FILE))\n",
        "    with open(META_FILE, \"wb\") as f:\n",
        "        pickle.dump(metadata, f)\n",
        "\n",
        "PARAPHRASE_PROMPT = \"\"\"You are a helpful assistant for GIKI students.\n",
        "Answer the user question using ONLY the provided context. Do NOT copy-paste sentences.\n",
        "Paraphrase and summarize in your own words. If the answer is not in the context, say you don't know.\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "def generate_answer_local(question: str, context: str, generator):\n",
        "    prompt = PARAPHRASE_PROMPT.format(question=question, context=context)\n",
        "    return generator(prompt)[0][\"generated_text\"].strip()\n",
        "\n",
        "def detect_lang_local(text: str) -> str:\n",
        "    try:\n",
        "        from langdetect import detect\n",
        "        code = detect(text)\n",
        "        return \"ur\" if code.startswith(\"ur\") else \"en\"\n",
        "    except:\n",
        "        for ch in text:\n",
        "            if \"\\u0600\" <= ch <= \"\\u06FF\":\n",
        "                return \"ur\"\n",
        "        return \"en\"\n",
        "\n",
        "def retrieve_context_local(query_en: str, index, metadata, embedder, top_k=4):\n",
        "    q_emb = embedder.encode([query_en], convert_to_numpy=True, normalize_embeddings=True)\n",
        "    D, I = index.search(q_emb, top_k)\n",
        "    chunks = []\n",
        "    for idx in I[0]:\n",
        "        if int(idx) in metadata:\n",
        "            chunks.append(metadata[int(idx)][\"text\"])\n",
        "    return \"\\\\n\\\\n\".join(chunks)\n",
        "\n",
        "def answer_question_local(query: str, lang, index, metadata, embedder, generator, top_k=4):\n",
        "    if lang == 'ur':\n",
        "        query_en = translate(query, \"ur-en\")\n",
        "    else:\n",
        "        query_en = query\n",
        "    ctx = retrieve_context_local(query_en, index, metadata, embedder, top_k=top_k)\n",
        "    ans_en = generate_answer_local(query_en, ctx, generator)\n",
        "    if lang == 'ur':\n",
        "        return translate(ans_en, \"en-ur\"), ctx\n",
        "    return ans_en, ctx\n",
        "\n",
        "st.set_page_config(page_title=\"GIKI RAG Chatbot\", page_icon=\"ðŸŽ“\", layout=\"wide\")\n",
        "st.title(\"ðŸŽ“ GIKI Prospectus Q&A (RAG)\")\n",
        "st.caption(\"Upload up to 5 docs (PDF/DOCX/TXT), build index once, then chat. English/Urdu supported.\")\n",
        "\n",
        "embedder = get_embedding_model()\n",
        "generator = get_generator()\n",
        "\n",
        "with st.sidebar:\n",
        "    st.header(\"Index Manager\")\n",
        "    uploaded = st.file_uploader(\"Upload up to 5 documents\", type=[\"pdf\",\"docx\",\"txt\"], accept_multiple_files=True)\n",
        "\n",
        "    # Add file size limit (5MB per file)\n",
        "    max_size_mb = 5\n",
        "    if uploaded:\n",
        "        valid_files = []\n",
        "        for f in uploaded[:5]:\n",
        "            if f.size > max_size_mb * 1024 * 1024:\n",
        "                st.warning(f\"File {f.name} is too large ({f.size//(1024*1024)}MB). Max size is {max_size_mb}MB.\")\n",
        "            else:\n",
        "                valid_files.append(f)\n",
        "\n",
        "        if valid_files and st.button(\"ðŸ”§ Build FAISS Index\"):\n",
        "            progress_bar = st.progress(0)\n",
        "            status_text = st.empty()\n",
        "\n",
        "            texts = []\n",
        "            for i, f in enumerate(valid_files):\n",
        "                status_text.text(f\"Processing {f.name} ({i+1}/{len(valid_files)})\")\n",
        "                # Create a temporary file\n",
        "                with tempfile.NamedTemporaryFile(delete=False, suffix=f.name) as tmp_file:\n",
        "                    tmp_file.write(f.read())\n",
        "                    tmp_path = tmp_file.name\n",
        "\n",
        "                if f.name.lower().endswith(\".pdf\"):\n",
        "                    txt = []\n",
        "                    # Limit PDF processing to first 20 pages\n",
        "                    with fitz.open(tmp_path) as doc:\n",
        "                        for page_num, page in enumerate(doc):\n",
        "                            if page_num >= 20:  # Limit to first 20 pages\n",
        "                                break\n",
        "                            txt.append(page.get_text())\n",
        "                    texts.append(\"\\\\n\".join(txt))\n",
        "                elif f.name.lower().endswith(\".docx\"):\n",
        "                    d = docx.Document(tmp_path)\n",
        "                    texts.append(\"\\\\n\".join([p.text for p in d.paragraphs]))\n",
        "                else:\n",
        "                    with open(tmp_path, encoding=\"utf-8\", errors=\"ignore\") as f_txt:\n",
        "                        texts.append(f_txt.read())\n",
        "\n",
        "                # Clean up temporary file\n",
        "                os.unlink(tmp_path)\n",
        "                progress_bar.progress((i+1)/len(valid_files))\n",
        "\n",
        "            full_text = \"\\\\n\".join(texts)\n",
        "            status_text.text(\"Chunking text...\")\n",
        "            chunks = chunk_text_local(full_text, 2000, 200)\n",
        "\n",
        "            status_text.text(\"Creating embeddings...\")\n",
        "            # Process in smaller batches with progress\n",
        "            batch_size = 8\n",
        "            all_embs = []\n",
        "\n",
        "            for i in range(0, len(chunks), batch_size):\n",
        "                batch = chunks[i:i+batch_size]\n",
        "                embs = embedder.encode(batch, batch_size=4, convert_to_numpy=True,\n",
        "                                      normalize_embeddings=True, show_progress_bar=False)\n",
        "                all_embs.append(embs)\n",
        "                progress_bar.progress(0.5 + 0.5 * (i / len(chunks)))\n",
        "\n",
        "            embs = np.vstack(all_embs)\n",
        "            dim = embs.shape[1]\n",
        "            index = faiss.IndexFlatIP(dim)\n",
        "            index.add(embs)\n",
        "            metadata = {i: {\"text\": chunks[i]} for i in range(len(chunks))}\n",
        "\n",
        "            status_text.text(\"Saving index...\")\n",
        "            save_faiss_local(index, metadata)\n",
        "            progress_bar.progress(1.0)\n",
        "            status_text.text(\"\")\n",
        "            st.success(f\"Index built with {len(chunks)} chunks!\")\n",
        "\n",
        "index, metadata = load_faiss_local()\n",
        "\n",
        "if index is None:\n",
        "    st.warning(\"Please upload documents and build the index first.\")\n",
        "    st.stop()\n",
        "\n",
        "st.success(f\"Index loaded with {len(metadata)} chunks!\")\n",
        "\n",
        "# Initialize session state\n",
        "if \"messages\" not in st.session_state:\n",
        "    st.session_state.messages = []\n",
        "\n",
        "# Language selection\n",
        "lang = st.radio(\"Language\", [\"English\", \"Urdu\"], horizontal=True, index=0)\n",
        "lang_code = \"en\" if lang == \"English\" else \"ur\"\n",
        "\n",
        "# Display chat messages\n",
        "for message in st.session_state.messages:\n",
        "    with st.chat_message(message[\"role\"]):\n",
        "        st.markdown(message[\"content\"])\n",
        "\n",
        "# Chat input\n",
        "if prompt := st.chat_input(\"Ask a question about GIKI...\"):\n",
        "    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "    with st.chat_message(\"user\"):\n",
        "        st.markdown(prompt)\n",
        "\n",
        "    with st.chat_message(\"assistant\"):\n",
        "        with st.spinner(\"Thinking...\"):\n",
        "            answer, context = answer_question_local(prompt, lang_code, index, metadata, embedder, generator)\n",
        "            st.markdown(answer)\n",
        "            with st.expander(\"View context used\"):\n",
        "                st.text(context)\n",
        "    st.session_state.messages.append({\"role\": \"assistant\", \"content\": answer})\n",
        "\n",
        "# Export conversation\n",
        "if st.session_state.messages:\n",
        "    if st.button(\"ðŸ’¾ Export Conversation to PDF\"):\n",
        "        lines = []\n",
        "        for msg in st.session_state.messages:\n",
        "            role = \"User\" if msg[\"role\"] == \"user\" else \"Assistant\"\n",
        "            lines.append(f\"{role}: {msg['content']}\")\n",
        "\n",
        "        filename = \"conversation.pdf\"\n",
        "        c = canvas.Canvas(filename, pagesize=letter)\n",
        "        width, height = letter\n",
        "        x, y = 40, height-40\n",
        "        wrap = 95\n",
        "        for line in lines:\n",
        "            for l in textwrap.wrap(line, wrap):\n",
        "                if y < 60:\n",
        "                    c.showPage()\n",
        "                    y = height-40\n",
        "                c.drawString(x, y, l)\n",
        "                y -= 14\n",
        "        c.save()\n",
        "\n",
        "        with open(filename, \"rb\") as f:\n",
        "            st.download_button(\"Download PDF\", f, file_name=filename, mime=\"application/pdf\")\n",
        "'''\n",
        "\n",
        "# Write the app file\n",
        "app_file = APP_DIR / \"app.py\"\n",
        "with open(app_file, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(app_content)\n",
        "\n",
        "print(\"Streamlit app created at:\", app_file)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "  !pip install --upgrade pyngrok"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hgDVmjLh2q-H",
        "outputId": "9756f285-b77c-4a40-8a3a-b1eae4fb63c3"
      },
      "id": "hgDVmjLh2q-H",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.12/dist-packages (7.3.0)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.12/dist-packages (from pyngrok) (6.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "  !pip install --upgrade streamlit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u-SfDievO-NJ",
        "outputId": "c32fc8a1-581d-4a6e-9264-27746914db12"
      },
      "id": "u-SfDievO-NJ",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.12/dist-packages (1.48.1)\n",
            "Requirement already satisfied: altair!=5.4.0,!=5.4.1,<6,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (8.2.1)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.0.2)\n",
            "Requirement already satisfied: packaging<26,>=20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (25.0)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (11.3.0)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.29.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.32.4)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (8.5.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.12/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (4.14.1)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.12/dist-packages (from streamlit) (3.1.45)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.12/dist-packages (from streamlit) (0.9.1)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (4.25.1)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2.1.2)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2025.8.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.27.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit pyngrok\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SM3WCOlTQw8v",
        "outputId": "e84e8b0d-a2b6-4275-db66-a78ad70cdf80"
      },
      "id": "SM3WCOlTQw8v",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.12/dist-packages (1.48.1)\n",
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.12/dist-packages (7.3.0)\n",
            "Requirement already satisfied: altair!=5.4.0,!=5.4.1,<6,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (8.2.1)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.0.2)\n",
            "Requirement already satisfied: packaging<26,>=20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (25.0)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (11.3.0)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.29.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.32.4)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (8.5.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.12/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (4.14.1)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.12/dist-packages (from streamlit) (3.1.45)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.12/dist-packages (from streamlit) (0.9.1)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (4.25.1)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2.1.2)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2025.8.3)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.12/dist-packages (from pyngrok) (6.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.27.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dcaed7c6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dcaed7c6",
        "outputId": "e7c45aae-dac9-40f3-8aa3-8ce04ca7b85f"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ngrok authtoken set from Colab secrets\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:pyngrok.process.ngrok:t=2025-08-26T17:38:34+0000 lvl=warn msg=\"ngrok config file found at both XDG and legacy locations, using XDG location\" xdg_path=/root/.config/ngrok/ngrok.yml legacy_path=/root/.ngrok2/ngrok.yml\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Public URL: https://4af6ca68a9ca.ngrok-free.app\n",
            "If runtime restarts, rerun this cell to get a new URL. Persist the FAISS index to avoid rebuilding.\n"
          ]
        }
      ],
      "source": [
        "# Launch Streamlit in Colab and open public URL via pyngrok\n",
        "import threading, subprocess, time, signal\n",
        "from pyngrok import ngrok, conf\n",
        "from pathlib import Path\n",
        "\n",
        "# Kill any existing ngrok processes\n",
        "ngrok.kill()\n",
        "\n",
        "# Set ngrok authtoken if available\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    ngrok.set_auth_token(userdata.get('NGROK_AUTH_TOKEN'))\n",
        "    print(\"Ngrok authtoken set from Colab secrets\")\n",
        "except:\n",
        "    print(\"No ngrok authtoken found in Colab secrets. Using free version.\")\n",
        "\n",
        "# Function to run Streamlit\n",
        "def run_streamlit():\n",
        "    import os\n",
        "    os.chdir(APP_DIR)\n",
        "    cmd = [\"streamlit\", \"run\", \"app.py\", \"--server.port\", \"8501\", \"--server.address\", \"0.0.0.0\", \"--server.headless\", \"true\"]\n",
        "    process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "    stdout, stderr = process.communicate()\n",
        "    print(\"Streamlit process ended\")\n",
        "    if stdout:\n",
        "        print(\"STDOUT:\", stdout.decode())\n",
        "    if stderr:\n",
        "        print(\"STDERR:\", stderr.decode())\n",
        "\n",
        "# Start Streamlit in a separate thread\n",
        "thread = threading.Thread(target=run_streamlit, daemon=True)\n",
        "thread.start()\n",
        "\n",
        "# Wait for Streamlit to start\n",
        "time.sleep(8)\n",
        "\n",
        "# Set up ngrok tunnel\n",
        "public_url = ngrok.connect(8501, bind_tls=True)\n",
        "print(\"Public URL:\", public_url.public_url)\n",
        "print(\"If runtime restarts, rerun this cell to get a new URL. Persist the FAISS index to avoid rebuilding.\")\n",
        "\n",
        "# Keep the process alive\n",
        "try:\n",
        "    while True:\n",
        "        time.sleep(10)\n",
        "except KeyboardInterrupt:\n",
        "    print(\"Shutting down...\")\n",
        "    ngrok.kill()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat /root/.streamlit/logs/*.log\n"
      ],
      "metadata": {
        "id": "u2pY51ihO5Nu"
      },
      "id": "u2pY51ihO5Nu",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}